{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YbJB2HywJ6Qi",
        "R-RqYx_7h_yt",
        "bbBo4h__kyEU",
        "m-IZlkdJiQRJ",
        "mru4l3INiiq0",
        "Ekmq26HJE9oA"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "dsnrro3KeApx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API Keys"
      ],
      "metadata": {
        "id": "FBZPQ7UkdCNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "financialmodelingprep_api_key = \"\"\n",
        "financialmodelingprep_host= 'https://financialmodelingprep.com/api/v3'"
      ],
      "metadata": {
        "id": "RdGCQkZDdEOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libs"
      ],
      "metadata": {
        "id": "2TxqgJJ-i0qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation of necessary libraries\n",
        "!pip3 install minisom\n",
        "!pip3 install tslearn\n",
        "!pip3 install pandas\n",
        "!pip3 install yfinance\n",
        "!pip3 install seaborn\n",
        "!pip3 install plotly\n",
        "!pip3 install pandas_datareader\n",
        "!pip3 install fastdtw\n",
        "\n",
        "# Import necessary libraries\n",
        "from joblib import Parallel, delayed\n",
        "from fastdtw import fastdtw\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns\n",
        "import plotly as pl\n",
        "import requests\n",
        "import datetime\n",
        "import sys\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.spatial.distance import euclidean\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "from plotly.offline import init_notebook_mode , iplot\n",
        "import plotly.graph_objs as go\n",
        "from minisom import MiniSom\n",
        "from tslearn.metrics import dtw_path\n",
        "from tslearn.barycenters import dtw_barycenter_averaging\n",
        "from tslearn.clustering import TimeSeriesKMeans\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from tslearn.neighbors import KNeighborsTimeSeriesClassifier\n",
        "from tslearn.metrics import dtw\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import pandas_datareader as pdr\n",
        "import yfinance as yf\n",
        "\n",
        "# Initialization\n",
        "sns.set_style(\"whitegrid\")\n",
        "init_notebook_mode(connected = True)"
      ],
      "metadata": {
        "id": "YJBueQ__WgOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from minisom import MiniSom\n",
        "import yfinance as yf  # To fetch stock data\n",
        "\n",
        "# Load stock data using yfinance\n",
        "tickers = [\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\"]\n",
        "start_date = \"2020-01-01\"\n",
        "end_date = \"2021-01-01\"\n",
        "data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
        "\n",
        "# Normalize data for better training of SOM\n",
        "normalized_data = (data - data.min()) / (data.max() - data.min())\n",
        "\n",
        "# Train a SOM\n",
        "som_dimension = 5\n",
        "som = MiniSom(som_dimension, som_dimension, normalized_data.shape[1], sigma=0.5, learning_rate=0.5)\n",
        "som.train_random(normalized_data.values, 1000)\n",
        "\n",
        "# Get the SOM's representation for each date\n",
        "winning_coordinates = np.array([som.winner(x) for x in normalized_data.values])\n",
        "\n",
        "# Prepare data for visualization\n",
        "mapped_data = pd.DataFrame(winning_coordinates, columns=['x', 'y'])\n",
        "mapped_data['Date'] = data.index\n",
        "\n",
        "mapped_data['Date'] = mapped_data['Date'].astype(str)\n",
        "\n",
        "# Visualize using Plotly\n",
        "fig = px.scatter(mapped_data, x='x', y='y', animation_frame='Date', range_color=[0, som_dimension-1])\n",
        "fig.write_html(\"som_plot.html\")\n",
        "\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=go.Scatter(x=mapped_data['x'], y=mapped_data['y'], mode='markers'))\n",
        "fig.update_layout(xaxis=dict(range=[mapped_data['x'].min(), mapped_data['x'].max()]),\n",
        "                  yaxis=dict(range=[mapped_data['y'].min(), mapped_data['y'].max()]))\n",
        "\n",
        "fig.write_html(\"som_plot2.html\")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "lsBCuXyle-xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "14UbtWvuWd3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive', force_remount=True)\n",
        "#base_data_path  = 'drive/MyDrive/Drive/MSc/Dissertation/StocksData/'\n",
        "\n",
        "#base_data_path  = '/Users/nenuadrian/Desktop/StocksData/'\n",
        "base_data_path = '/'\n",
        "\n",
        "if not os.path.isdir(f\"{base_data_path}stock_prices\"):\n",
        "  os.mkdir(f\"{base_data_path}stock_prices\")\n",
        "\n",
        "initial_stock_prices = None\n",
        "dtw_cache = False"
      ],
      "metadata": {
        "id": "J2NB5-d-b33F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FinancialModelingPrep Profile Data"
      ],
      "metadata": {
        "id": "q0vwQKTOc0d2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Companies: NASDAQ & S&P"
      ],
      "metadata": {
        "id": "XWtebJbahRib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_companies(path, api):\n",
        "  companies_path = f\"{base_data_path}{path}.csv\"\n",
        "  if not os.path.isfile(companies_path):\n",
        "    response = requests.get(f'{financialmodelingprep_host}/{api}?apikey={financialmodelingprep_api_key}')\n",
        "    companies = pd.DataFrame.from_records(response.json())\n",
        "    companies.to_csv(companies_path)\n",
        "    print(f\"Downloaded {path} companies\")\n",
        "  else:\n",
        "    print(f\"Loading {path} companies from cache\")\n",
        "    companies = pd.read_csv(companies_path)\n",
        "\n",
        "  print(f\"{len(companies)} {path} companies loaded\")\n",
        "  return companies\n"
      ],
      "metadata": {
        "id": "aP_JmV75B3Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_companies = load_companies('companies_sp', 'sp500_constituent')\n",
        "nasdaq_companies = load_companies('companies_nasdaq', 'nasdaq_constituent')"
      ],
      "metadata": {
        "id": "-_4zN0zdha8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Company profiles\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aSqRrWuMhUUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_symbols = np.concatenate([nasdaq_companies[\"symbol\"].values, sp_companies[\"symbol\"].values])\n",
        "print(f\"Identified {len(initial_symbols)} NASDAQ + S&P symbols\")\n",
        "initial_symbols = list(set(initial_symbols))\n",
        "print(f\"Identified {len(initial_symbols)} unique symbols\")"
      ],
      "metadata": {
        "id": "WAeiaEMElOF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "companies_profiles_path = f\"{base_data_path}companies_profiles.csv\"\n",
        "if not os.path.isfile(companies_profiles_path):\n",
        "  companies_profiles = None\n",
        "  for index, stock_symbol in enumerate(initial_symbols):\n",
        "    if companies_profiles is None or companies_profiles.empty or len(companies_profiles.loc[companies_profiles['symbol'] == stock_symbol]) == 0:\n",
        "      # Make the request to the API\n",
        "      print(f\"Downloaded data for {stock_symbol} {index}/{len(initial_symbols)}\")\n",
        "      response = requests.get(f'{financialmodelingprep_host}/profile/{stock_symbol}?apikey={financialmodelingprep_api_key}')\n",
        "      if companies_profiles is None:\n",
        "        companies_profiles = [response.json()[0]]\n",
        "        companies_profiles = pd.DataFrame.from_records(companies_profiles)\n",
        "      else:\n",
        "        row = pd.DataFrame.from_records([response.json()[0]])\n",
        "        companies_profiles = pd.concat([companies_profiles, row])\n",
        "  companies_profiles.to_csv(companies_profiles_path)\n",
        "else:\n",
        "  companies_profiles = pd.read_csv(companies_profiles_path)\n",
        "\n",
        "companies_profiles = {\n",
        "    stock_symbol: companies_profiles.loc[companies_profiles['symbol'] == stock_symbol] for stock_symbol in initial_symbols\n",
        "}\n",
        "\n",
        "print(f\"{len(companies_profiles)} loaded company profiles\")"
      ],
      "metadata": {
        "id": "flhkwXt8_6X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Historical Stock Prices"
      ],
      "metadata": {
        "id": "ZxHXqJ1dol9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prices_path = f\"{base_data_path}stock_prices/\"\n",
        "start = '2015-01-01'\n",
        "end = '2023-01-01'\n",
        "for index, stock_symbol in enumerate(initial_symbols):\n",
        "  stock_path = f\"{prices_path}{stock_symbol}.csv\"\n",
        "  if not os.path.isfile(stock_path):\n",
        "    print(f\"Downloaded data for {stock_symbol} {index}/{len(initial_symbols)}\")\n",
        "    response = requests.get(f'{financialmodelingprep_host}/historical-price-full/{stock_symbol}?apikey={financialmodelingprep_api_key}&from={start}&to={end}')\n",
        "    data = pd.DataFrame.from_records(response.json()['historical'])\n",
        "    data.to_csv(stock_path)"
      ],
      "metadata": {
        "id": "cMDf4yM9orHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse Stock Files"
      ],
      "metadata": {
        "id": "yULhedTNh2kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prices_path = f\"{base_data_path}stock_prices/\"\n",
        "\n",
        "if not initial_stock_prices:\n",
        "  print(\"Parsing files\")\n",
        "  initial_stock_prices = []\n",
        "  for stock_symbol in initial_symbols:\n",
        "    stock_path = f\"{prices_path}{stock_symbol}.csv\"\n",
        "    stock_price = pd.read_csv(stock_path)\n",
        "    stock_price = stock_price.loc[:,[\"date\",\"close\"]]\n",
        "    stock_price.set_index(\"date\",inplace=True)\n",
        "    stock_price.sort_index(inplace=True)\n",
        "    initial_stock_prices.append(stock_price)\n",
        "\n",
        "print(f\"Files parsed from '{prices_path}': {len(initial_stock_prices)}\")"
      ],
      "metadata": {
        "id": "0oascNB2sJT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-processing"
      ],
      "metadata": {
        "id": "P1ZduRwexV7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Narrowing down the data if configured"
      ],
      "metadata": {
        "id": "gkVFP1vl6aXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "symbols = initial_symbols\n",
        "stock_prices = initial_stock_prices\n",
        "\n",
        "print(f\"Symbols left: {len(symbols)} - stock prices: {len(stock_prices)}\")"
      ],
      "metadata": {
        "id": "KUT5EXLb6ipC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check for missing data"
      ],
      "metadata": {
        "id": "CnNRtY5gDEX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nan_counter(list_of_series):\n",
        "    nan_polluted_series_counter = 0\n",
        "    for series in list_of_series:\n",
        "        if series.isnull().sum().sum() > 0:\n",
        "            nan_polluted_series_counter+=1\n",
        "    return nan_polluted_series_counter\n",
        "\n",
        "print(f\"Missing data points: {nan_counter(stock_prices)}\")"
      ],
      "metadata": {
        "id": "pjIq_KssxY2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filter out companies with less than 2000 prices"
      ],
      "metadata": {
        "id": "ANx4TTdHC_P5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "symbols_without_enough_data = []\n",
        "\n",
        "for index, symbol in enumerate(symbols):\n",
        "  if len(stock_prices[index]) < 2000:\n",
        "    symbols_without_enough_data.append(symbol)\n",
        "\n",
        "stock_prices = [prices for prices in stock_prices if len(prices) >= 2000]\n",
        "\n",
        "print(f\"Companies without enough data {len(symbols_without_enough_data)}\")\n",
        "\n",
        "for symbol in symbols_without_enough_data:\n",
        "  symbols.remove(symbol)\n",
        "\n",
        "companies_profiles = {symbol: companies_profiles[symbol] for symbol in symbols}\n",
        "\n",
        "print(f\"Remaining cleaned symbols {len(symbols)} - stock_prices {len(stock_prices)} - profiles {len(companies_profiles)}\")"
      ],
      "metadata": {
        "id": "QQl4gh-Wz4OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identify a window of time for which all companies have data"
      ],
      "metadata": {
        "id": "mT2OOE0KDI7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_start = '2015-01-01'\n",
        "max_end = '2020-01-01'\n",
        "\n",
        "for prices in stock_prices:\n",
        "  if len(prices) > 2000:\n",
        "    min_start = max(prices.index[0], min_start)\n",
        "    max_end = min(prices.index[-1], max_end)\n",
        "\n",
        "print(min_start)\n",
        "print(max_end)"
      ],
      "metadata": {
        "id": "Sn9U6pKNxKgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keep only data within the cross-over window"
      ],
      "metadata": {
        "id": "hDlS3MppDPGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total series before filtering for required timeframe: {len(stock_prices)}\")\n",
        "\n",
        "for index, prices in enumerate(stock_prices):\n",
        "  stock_prices[index] = prices.filter(items = [date for date in prices.index if date >= min_start and date <= max_end], axis=0)\n",
        "\n",
        "print(f\"Total series after filtering for required timeframe: {len(stock_prices)}\")"
      ],
      "metadata": {
        "id": "Xq33KDngxDLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize"
      ],
      "metadata": {
        "id": "k06czamVh__I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_prices_normal = []\n",
        "scaler = MinMaxScaler()\n",
        "for prices in stock_prices:\n",
        "  df = prices.copy(deep = True)\n",
        "  df = MinMaxScaler().fit_transform(df)\n",
        "  df = df.reshape(len(df))\n",
        "  stock_prices_normal.append(df)\n",
        "\n",
        "print(f\"Normalised stocks: {len(stock_prices_normal)}\")"
      ],
      "metadata": {
        "id": "0J4CNNmPwvX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore"
      ],
      "metadata": {
        "id": "Fi-j3S6ACYCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Industries, sectors and countries"
      ],
      "metadata": {
        "id": "hWEkOP_peY6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "industries = [companies_profiles[symbol][\"industry\"].values[0] for symbol in companies_profiles]\n",
        "sectors = [companies_profiles[symbol][\"sector\"].values[0] for symbol in companies_profiles]\n",
        "countries = [companies_profiles[symbol][\"country\"].values[0] for symbol in companies_profiles]"
      ],
      "metadata": {
        "id": "mJ1Ijl3httRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sectorsDf = pd.DataFrame(sectors,columns=[\"Sector\"]).groupby(\"Sector\")[\"Sector\"].count().sort_values(ascending=False).reset_index(name='count')\n",
        "sectorsDf"
      ],
      "metadata": {
        "id": "IRaVB7UvnsXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "industriesDf = pd.DataFrame(industries,columns=[\"Industry\"]).groupby(\"Industry\")[\"Industry\"].count().sort_values(ascending=False).reset_index(name='count')\n",
        "\n",
        "industriesDf\n"
      ],
      "metadata": {
        "id": "R2OHop4lnZsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sectorsGroup = pd.DataFrame(sectors,columns=[\"sector\"]).groupby(\"sector\")[\"sector\"].count().sort_values(ascending=False).reset_index(name='count')\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.rcParams['font.size'] = 14\n",
        "\n",
        "plt.pie(sectorsGroup[\"count\"], labels = sectorsGroup[\"sector\"], shadow = True)\n",
        "plt.show()\n",
        "sectorsGroup"
      ],
      "metadata": {
        "id": "QfschFElutx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "countries = pd.DataFrame(countries,columns=[\"country\"]).groupby(\"country\")[\"country\"].count().sort_values(ascending=False).reset_index(name='count')\n",
        "countries"
      ],
      "metadata": {
        "id": "ODtbQvSNux8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stocks"
      ],
      "metadata": {
        "id": "AbtdaKPHeW1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plots = stock_prices[:30]\n",
        "\n",
        "fig, axs = plt.subplots(math.floor(len(plots) / 3),3,figsize=(25,45))\n",
        "for i in range(math.floor(len(plots) / 3) + 1):\n",
        "  for j in range(3):\n",
        "      if i*3+j+1>len(plots): # pass the others that we can't fill\n",
        "          continue\n",
        "      axs[i, j].plot(plots[i*3+j].values)\n",
        "      axs[i, j].set_title(symbols[i*3+j])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4s1kedITBdsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig = plt.figure(figsize=(25,5))\n",
        "for stock in stock_prices:\n",
        "  plt.plot(stock)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hnw46SGurMBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(25,5))\n",
        "for stock in stock_prices_normal:\n",
        "  plt.plot(stock)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WzSeljZ8tDeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DTW Paths"
      ],
      "metadata": {
        "id": "4GqKxKZ4eh3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock1 = stock_prices_normal[0]\n",
        "stock2 = stock_prices_normal[30]\n",
        "\n",
        "path, sim = dtw_path(stock1, stock2)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "# Plot stock1\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(stock1, label='Stock 1')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# Plot stock2\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(stock2, label='Stock 2')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# Plot DTW path\n",
        "plt.subplot(2, 2, 2)\n",
        "for (i, j) in path:\n",
        "    plt.plot([i, j], [stock1[i], stock2[j]], color='gray', linewidth=0.5)\n",
        "\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qfl1B3uielqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a matrix with zeros\n",
        "mat = np.zeros((len(stock1), len(stock2)))\n",
        "\n",
        "# Update the matrix cells to one, corresponding to the DTW path\n",
        "for i, j in path:\n",
        "    mat[i, j] = 1\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "# Show the matrix\n",
        "cax = ax.matshow(mat.T, origin='lower', cmap='gray_r')\n",
        "fig.colorbar(cax)\n",
        "\n",
        "plt.plot([j for (i, j) in path], [i for (i, j) in path], \"w-\", linewidth=2.)  # DTW path\n",
        "\n",
        "plt.title('DTW Warping Path')\n",
        "plt.xlabel('Stock 1')\n",
        "plt.ylabel('Stock 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cGbQHZQZigQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock1 = np.random.rand(100)\n",
        "stock2 = np.random.rand(100) + 1   # just to make the two series visibly different\n",
        "path, sim = dtw_path(stock1, stock2)\n",
        "\n",
        "# Plot the two time series\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(stock1, 'r-', label='stock1')\n",
        "plt.plot(stock2, 'g-', label='stock2')\n",
        "\n",
        "# Draw lines between matched points according to the warping path\n",
        "for (i, j) in path:\n",
        "    plt.plot([i, j], [stock1[i], stock2[j]], 'b-', linewidth=0.1)\n",
        "\n",
        "plt.legend()\n",
        "plt.title('Dynamic Time Warping')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LvQN77mqjE17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume these are your time series\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(2,1,1)\n",
        "\n",
        "plt.plot(stock1, 'r', label='stock1')\n",
        "plt.plot(stock2, 'g', label='stock2')\n",
        "plt.legend()\n",
        "\n",
        "# Creating the connections for the Euclidean distance\n",
        "for i in range(len(stock1)):\n",
        "    plt.plot([i, i], [stock1[i], stock2[i]], 'b')\n",
        "\n",
        "plt.title('Euclidean Distance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XBsf2I1RteOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis"
      ],
      "metadata": {
        "id": "qHSimTcGWjrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation Heatmap"
      ],
      "metadata": {
        "id": "wIDLP4jpSVPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming each stock has the same dates\n",
        "close_prices = pd.concat([stock_prices[i]['close'] for i in range(len(stock_prices))], axis=1)\n",
        "close_prices.columns = ['Stock {}'.format(i) for i in range(len(stock_prices))]\n",
        "correlation_matrix = close_prices.corr()\n",
        "correlation_matrix"
      ],
      "metadata": {
        "id": "hA_9EMOGTPzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have `stock_prices` as a list of dataframes\n",
        "close_prices = pd.concat([stock_prices[i]['close'] for i in range(100)], axis=1)\n",
        "close_prices.columns = ['Stock {}'.format(i + 1) for i in range(100)]\n",
        "correlation_matrix = close_prices.corr()\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(correlation_matrix, cmap=\"coolwarm\", center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .75})\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kwINQpCxw6P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plt.figure(figsize=(14,7))\n",
        "#sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "#plt.title('Correlation Heatmap of Closing Prices for All Stocks')\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "1hM1VV_HSYKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Average vs DBA"
      ],
      "metadata": {
        "id": "BJz-fz4Gfp5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tslearn.barycenters import dtw_barycenter_averaging\n",
        "\n",
        "# Generate synthetic stock data with different scales\n",
        "np.random.seed(0)  # for reproducibility\n",
        "time_points = 100\n",
        "base = np.linspace(100, 150, time_points)\n",
        "\n",
        "# First stock: Small fluctuations around the base\n",
        "stock1 = base + np.random.randn(time_points) * 2\n",
        "\n",
        "# Second stock: Sinusoidal fluctuations of larger magnitude around the base\n",
        "stock2 = base + np.random.randn(time_points) * 2 + np.sin(np.linspace(0, 4 * np.pi, time_points)) * 20\n",
        "\n",
        "# Third stock: Linear trend that is drastically different from the base\n",
        "stock3 = base + np.linspace(-50, 50, time_points) + np.random.randn(time_points) * 2\n",
        "\n",
        "stocks = np.array([stock1, stock2, stock3])\n",
        "\n",
        "# Compute simple average\n",
        "simple_avg = np.mean(stocks, axis=0)\n",
        "\n",
        "# Compute DTW Barycenter Averaging (DBA)\n",
        "dba_avg = dtw_barycenter_averaging(stocks)\n",
        "\n",
        "# Visualization using matplotlib.pyplot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot individual stocks\n",
        "colors = ['blue', 'orange', 'purple']\n",
        "for i, stock in enumerate(stocks):\n",
        "    plt.plot(stock, label=f'Stock {i+1}', color=colors[i], linewidth=1.5)\n",
        "\n",
        "# Plot simple average\n",
        "plt.plot(simple_avg, label='Simple Average', color='red', linewidth=2, linestyle='dotted')\n",
        "\n",
        "# Plot DBA\n",
        "plt.plot(dba_avg.ravel(), label='DBA Average', color='green', linewidth=2, linestyle='dashed')\n",
        "\n",
        "plt.legend()\n",
        "plt.title(\"Stock Prices: Simple Average vs. DBA\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Stock Price\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "88m7-cZYHHJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualization using matplotlib.pyplot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot individual stocks\n",
        "colors = ['blue', 'orange', 'purple']\n",
        "for i, stock in enumerate(stocks):\n",
        "    plt.plot(stock, label=f'Stock {i+1}', color=colors[i], linewidth=1.5)\n",
        "\n",
        "# Plot simple average\n",
        "plt.plot(simple_avg, label='Simple Average', color='red', linewidth=2, linestyle='dotted')\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "plt.title(\"Simple Average of Stock prices\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Stock Price\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RLnpKFBzYt2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(0)\n",
        "cluster1 = np.random.randn(50, 2) + [2, 2]\n",
        "cluster2 = np.random.randn(50, 2) + [7, 7]\n",
        "cluster3 = np.random.randn(50, 2) + [2, 7]\n",
        "\n",
        "data = np.vstack([cluster1, cluster2, cluster3])\n",
        "\n",
        "# Compute centroids\n",
        "centroid1 = np.mean(cluster1, axis=0)\n",
        "centroid2 = np.mean(cluster2, axis=0)\n",
        "centroid3 = np.mean(cluster3, axis=0)\n",
        "\n",
        "global_centroid = np.mean(data, axis=0)\n",
        "\n",
        "# Plot clusters\n",
        "plt.scatter(cluster1[:, 0], cluster1[:, 1], color='blue', s=10, label='Cluster 1')\n",
        "plt.scatter(cluster2[:, 0], cluster2[:, 1], color='orange', s=10, label='Cluster 2')\n",
        "plt.scatter(cluster3[:, 0], cluster3[:, 1], color='green', s=10, label='Cluster 3')\n",
        "\n",
        "# Plot centroids\n",
        "plt.scatter([centroid1[0], centroid2[0], centroid3[0]],\n",
        "            [centroid1[1], centroid2[1], centroid3[1]],\n",
        "            color='black', s=50, marker='x', label='Centroids')\n",
        "\n",
        "# Plot global centroid\n",
        "plt.scatter(global_centroid[0], global_centroid[1], color='red', s=50, marker='x', label='Global Centroid')\n",
        "\n",
        "# Draw lines to global centroid\n",
        "for centroid in [centroid1, centroid2, centroid3]:\n",
        "    plt.plot([centroid[0], global_centroid[0]], [centroid[1], global_centroid[1]], 'r--')\n",
        "\n",
        "plt.title(\"Calinski-Harabasz Index\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zPBu1t3tGlAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "k35M2uspgJit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_with_average(labels, k):\n",
        "  plot_count = math.ceil(math.sqrt(k))\n",
        "  fig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25))\n",
        "  row_i=0\n",
        "  column_j=0\n",
        "  # For each label there is,\n",
        "  # plots every series with that label\n",
        "  for label in set(labels):\n",
        "    cluster = []\n",
        "    for i in range(len(labels)):\n",
        "      if(labels[i]==label):\n",
        "          axs[row_i, column_j].plot(stock_prices_normal[i],c=\"gray\",alpha=0.4)\n",
        "          cluster.append(stock_prices_normal[i])\n",
        "    if len(cluster) > 0:\n",
        "        axs[row_i, column_j].plot(np.average(np.vstack(cluster),axis=0),c=\"red\")\n",
        "    axs[row_i, column_j].set_title(\"Cluster \"+str(row_i*plot_count+column_j + 1))\n",
        "    column_j+=1\n",
        "    if column_j%plot_count == 0:\n",
        "        row_i+=1\n",
        "        column_j=0\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def compute_dtw_barycenter_averaging(cluster, data):\n",
        "  return [cluster, dtw_barycenter_averaging(np.vstack(data))]\n",
        "\n",
        "def draw_with_bary(labels, k):\n",
        "  plot_count = math.ceil(math.sqrt(k))\n",
        "\n",
        "  fig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25))\n",
        "  row_i=0\n",
        "  column_j=0\n",
        "  for label in set(clusters_kmeans_dtw):\n",
        "      cluster = []\n",
        "      for i in range(len(clusters_kmeans_dtw)):\n",
        "              if(clusters_kmeans_dtw[i]==label):\n",
        "                  axs[row_i, column_j].plot(stock_prices_normal[i],c=\"gray\",alpha=0.4)\n",
        "                  cluster.append(stock_prices_normal[i])\n",
        "      if len(cluster) > 0:\n",
        "          axs[row_i, column_j].plot(dtw_barycenter_averaging(np.vstack(cluster)),c=\"red\")\n",
        "      axs[row_i, column_j].set_title(\"Cluster \"+str(row_i*plot_count+column_j + 1))\n",
        "      column_j+=1\n",
        "      if column_j%plot_count == 0:\n",
        "          row_i+=1\n",
        "          column_j=0\n",
        "\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "tH3Qna6HgLvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DTW Flat Clusters From Hierarchical Clustering"
      ],
      "metadata": {
        "id": "oh2I_K7hOEAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run - PARALLEL"
      ],
      "metadata": {
        "id": "VireJ_QkOHHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from scipy.spatial.distance import squareform\n",
        "\n",
        "\n",
        "def compute_dtw(i, j):\n",
        "  return [[i, j], dtw(stock_prices_normal[i], stock_prices_normal[j])]\n",
        "\n",
        "dtw_matrix_path = f\"{base_data_path}dtw_matrix.json\"\n",
        "\n",
        "\n",
        "to_process = []\n",
        "n = len(stock_prices_normal)  # Number of stocks\n",
        "for i in range(n):\n",
        "  for j in range(n):\n",
        "    if i != j:\n",
        "      to_process.append([i, j])\n",
        "\n",
        "results = Parallel(n_jobs=5)(delayed(compute_dtw)(x[0], x[1]) for x in to_process)\n",
        "\n",
        "dtw_matrix = np.zeros((n, n))\n",
        "for result in results:\n",
        "  dtw_matrix[result[0][0], result[0][1]] = result[1]\n",
        "\n"
      ],
      "metadata": {
        "id": "oVLx2HMuOGn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from json import JSONEncoder\n",
        "\n",
        "class NumpyArrayEncoder(JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        return JSONEncoder.default(self, obj)\n",
        "\n",
        "dtw_matrix_path = f\"{base_data_path}dtw_matrix.json\"\n",
        "\n",
        "# Serializing json\n",
        "json_object = json.dumps(dtw_matrix, cls=NumpyArrayEncoder)\n",
        "\n",
        "# Writing to sample.json\n",
        "with open(dtw_matrix_path, \"w\") as outfile:\n",
        "    outfile.write(json_object)"
      ],
      "metadata": {
        "id": "WWNWoQersXC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for method in ['average', 'ward']:\n",
        "  # Perform hierarchical clustering\n",
        "  condensed_dist_matrix = squareform(dtw_matrix)\n",
        "  Z = linkage(condensed_dist_matrix, method=method)\n",
        "\n",
        "  # Create a dendrogram\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  dendrogram(Z)\n",
        "  plt.title(f'Hierarchical DTW Clustering Dendrogram {method}')\n",
        "  plt.xlabel('Stock')\n",
        "  plt.ylabel('Distance')\n",
        "  plt.show()\n",
        "\n",
        "  hierarchical_k = 23\n",
        "  hierarchical_labels = fcluster(Z, hierarchical_k, criterion='maxclust')\n",
        "  draw_with_average(hierarchical_labels, hierarchical_k)\n",
        "\n",
        "  print(\"Silhouette Coefficient: %0.3f\"\n",
        "      % metrics.silhouette_score(dtw_matrix, hierarchical_labels, metric=\"precomputed\"))\n",
        "\n",
        "  print(\"Calinski-Harabasz Index: %0.3f\"\n",
        "        % metrics.calinski_harabasz_score(stock_prices_normal, hierarchical_labels))\n",
        "\n",
        "  print(\"Davies-Bouldin Index: %0.3f\"\n",
        "        % metrics.davies_bouldin_score(stock_prices_normal, hierarchical_labels))\n",
        "\n",
        "\n",
        "  cluster_map = []\n",
        "  for idx in range(len(hierarchical_labels)):\n",
        "      winner_node = hierarchical_labels[idx]\n",
        "      industry = companies_profiles[symbols[idx]][\"industry\"].values[0]\n",
        "      cluster_map.append((winner_node , industry))\n",
        "\n",
        "\n",
        "  pivot_df = pd.DataFrame(cluster_map,columns=[\"Cluster\", \"Industry\"]).sort_values(by=\"Cluster\")\n",
        "\n",
        "  # Count unique industries per cluster\n",
        "  unique_industries_per_cluster = pivot_df.groupby(\"Cluster\")[\"Industry\"].nunique().reset_index().sort_values(by=\"Cluster\")\n",
        "\n",
        "  # Plotting the data using matplotlib.pyplot\n",
        "  plt.figure(figsize=(25,5))\n",
        "  plt.bar(unique_industries_per_cluster[\"Cluster\"], unique_industries_per_cluster[\"Industry\"],)\n",
        "  plt.xlabel(\"Cluster\")\n",
        "  plt.ylabel(\"Number of Unique Industries\")\n",
        "  plt.title(\"Number of Unique Industries per Cluster\")\n",
        "  plt.xticks(unique_industries_per_cluster[\"Cluster\"].tolist())\n",
        "  plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  cluster_c = [len(hierarchical_labels[hierarchical_labels==i]) for i in range(1, 24)]\n",
        "  cluster_n = [f\"C{i}\" for i in range(1, 24)]\n",
        "  plt.figure(figsize=(25,5))\n",
        "  plt.title(\"Cluster Distribution of stocks for DTW Kernel\")\n",
        "  plt.bar(cluster_n,cluster_c)\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "ARo_FmmYZuQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ],
      "metadata": {
        "id": "FVAMuggynizA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for cluster in [5]:\n",
        "  plt.figure(figsize=(25,5))\n",
        "  plt.title(f\"Cluster {cluster}\")\n",
        "  for index, clusterNo in enumerate(clusters):\n",
        "    if clusterNo == cluster:\n",
        "      plt.plot(stock_prices_normal[index])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "qIqDo9eYlpto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MiniSom\n"
      ],
      "metadata": {
        "id": "4580Ui_xE6Ab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run"
      ],
      "metadata": {
        "id": "BNpbj00l9YlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def print_stats_som(mini_som_clusters):\n",
        "  print(\"Silhouette Coefficient: %0.3f\"\n",
        "        % metrics.silhouette_score(stock_prices_normal, mini_som_clusters))\n",
        "\n",
        "  print(\"Calinski-Harabasz Index: %0.3f\"\n",
        "        % metrics.calinski_harabasz_score(stock_prices_normal, mini_som_clusters))\n",
        "\n",
        "  print(\"Davies-Bouldin Index: %0.3f\"\n",
        "        % metrics.davies_bouldin_score(stock_prices_normal, mini_som_clusters))\n",
        "\n",
        "  return [metrics.silhouette_score(stock_prices_normal, mini_som_clusters),\n",
        "          metrics.calinski_harabasz_score(stock_prices_normal, mini_som_clusters),\n",
        "          metrics.davies_bouldin_score(stock_prices_normal, mini_som_clusters)]\n",
        "\n",
        "for sigma in list(np.arange(0.1, 1, .1)):\n",
        "  print(f\"Sigma: {sigma}\")\n",
        "  som_x = som_y = math.ceil(math.sqrt(math.sqrt(len(stock_prices_normal))))\n",
        "\n",
        "  som = MiniSom(som_x, som_y, len(stock_prices_normal[0]), sigma=sigma, learning_rate = 0.3, random_seed=1)\n",
        "\n",
        "  som.random_weights_init(stock_prices_normal)\n",
        "  som.train(stock_prices_normal, 50000)\n",
        "\n",
        "  # Calculate the quantization error\n",
        "  q_error = som.quantization_error(stock_prices_normal)\n",
        "  print(\"Quantization error:\", q_error)\n",
        "\n",
        "  # Calculate the topographic error\n",
        "  t_error = som.topographic_error(stock_prices_normal)\n",
        "  print(\"Topographic error:\", t_error)\n",
        "\n",
        "  # Calculate the U-matrix\n",
        "  u_matrix = som.distance_map()\n",
        "  print(\"U-matrix:\", u_matrix)\n",
        "\n",
        "  win_map = som.win_map(stock_prices_normal)\n",
        "\n",
        "  mini_som_clusters = []\n",
        "  for idx in range(len(stock_prices_normal)):\n",
        "      winner_node = som.winner(stock_prices_normal[idx])\n",
        "      mini_som_clusters.append(winner_node[0]*som_y+winner_node[1]+1)\n",
        "\n",
        "  print_stats_som(mini_som_clusters)\n"
      ],
      "metadata": {
        "id": "Up5fy3TcDNVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then calculate the quantization error of the model using the quantization_error method and print it to the console. The quantization error measures how well the model preserves the distances between the input data points.\n",
        "\n",
        "We also calculate the topographic error of the model using the topographic_error method and print it to the console. The topographic error measures how well the model preserves the topology of the input data, i.e. how well it preserves the relationships between nearby data points.\n",
        "\n",
        "Finally, we calculate the U-matrix of the model using the distance_map method and print it to the console. The U-matrix is a 2D array that visualizes the distances between adjacent nodes on the grid. It can be used to identify clusters of similar data points."
      ],
      "metadata": {
        "id": "1dXmtH5m9cpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ],
      "metadata": {
        "id": "tooVhAqLIpKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "som_results_stats = [print_stats_som(result) for result in som_results]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df = MinMaxScaler().fit_transform(pd.DataFrame([result[0] for result in som_results_stats]))\n",
        "df1 = df.reshape(len(df))\n",
        "\n",
        "df = MinMaxScaler().fit_transform(pd.DataFrame([result[1] for result in som_results_stats]))\n",
        "df2 = df.reshape(len(df))\n",
        "\n",
        "df = MinMaxScaler().fit_transform(pd.DataFrame([result[2] for result in som_results_stats]))\n",
        "df3 = df.reshape(len(df))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(3, 30), df1, label='Silhouette by Coefficient')\n",
        "plt.plot(range(3, 30), df2, label='Calinski-Harabasz Index')\n",
        "plt.plot(range(3, 30), df3, label='Davies-Bouldin Index')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1ti_t3m291yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore"
      ],
      "metadata": {
        "id": "6HxJyuklzhtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clusters"
      ],
      "metadata": {
        "id": "YbJB2HywJ6Qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for cluster in [1]:\n",
        "  plt.figure(figsize=(25,5))\n",
        "  plt.title(f\"Cluster {cluster}\")\n",
        "  for index, clusterNo in enumerate(mini_som_clusters):\n",
        "    if clusterNo == cluster:\n",
        "      plt.plot(stock_prices_normal[index])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "I0yHwa5PJ7j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Averaging Centre"
      ],
      "metadata": {
        "id": "R-RqYx_7h_yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Little handy function to plot series\n",
        "def plot_som_series_averaged_center(som_x, som_y, win_map):\n",
        "    fig, axs = plt.subplots(som_x,som_y,figsize=(25,25))\n",
        "    for x in range(som_x):\n",
        "        for y in range(som_y):\n",
        "            cluster = (x,y)\n",
        "            if cluster in win_map.keys():\n",
        "                for series in win_map[cluster]:\n",
        "                    axs[cluster].plot(series,c=\"gray\",alpha=0.5)\n",
        "                axs[cluster].plot(np.average(np.vstack(win_map[cluster]),axis=0),c=\"red\")\n",
        "            cluster_number = x*som_y+y+1\n",
        "            axs[cluster].set_title(f\"Cluster {cluster_number}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Returns the mapping of the winner nodes and inputs\n",
        "print(f\"Clusters: {len(win_map)}\")\n",
        "plot_som_series_averaged_center(som_x, som_y, win_map)"
      ],
      "metadata": {
        "id": "c8LJEajgDwfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DTW Barycenter Averaging (DBA) - PARALLEL PROCESSING"
      ],
      "metadata": {
        "id": "H3czkQ5LiB0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "to_process = []\n",
        "for x in range(som_x):\n",
        "  for y in range(som_y):\n",
        "    cluster = (x,y)\n",
        "    if cluster in win_map.keys():\n",
        "      to_process.append([cluster, win_map[cluster]])\n",
        "\n",
        "numbers = Parallel(n_jobs=5)(delayed(compute_dtw_barycenter_averaging)(x[0], x[1]) for x in to_process)"
      ],
      "metadata": {
        "id": "Jo7NlePzFS_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(som_x,som_y,figsize=(25,25))\n",
        "for data in numbers:\n",
        "  cluster = data[0]\n",
        "  for series in win_map[cluster]:\n",
        "    axs[cluster].plot(series,c=\"gray\",alpha=0.5)\n",
        "  axs[cluster].plot(data[1],c=\"red\")\n",
        "  cluster_number = data[0][0]*som_y+data[0][1]+1\n",
        "  axs[cluster].set_title(f\"Cluster {cluster_number}\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(f\"Clusters: {len(win_map)}\")"
      ],
      "metadata": {
        "id": "tOV_5wPpD7fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_c = []\n",
        "cluster_n = []\n",
        "for x in range(som_x):\n",
        "    for y in range(som_y):\n",
        "        cluster = (x,y)\n",
        "        if cluster in win_map.keys():\n",
        "            cluster_c.append(len(win_map[cluster]))\n",
        "        else:\n",
        "            cluster_c.append(0)\n",
        "        cluster_number = x*som_y+y+1\n",
        "        cluster_n.append(f\"{cluster_number}\")\n",
        "\n",
        "plt.figure(figsize=(25,5))\n",
        "plt.title(\"Cluster Distribution for SOM\")\n",
        "plt.bar(cluster_n,cluster_c)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eqQTk2XSEHE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_map = []\n",
        "for idx in range(len(stock_prices_normal)):\n",
        "    winner_node = som.winner(stock_prices_normal[idx])\n",
        "    industry = companies_profiles[symbols[idx]][\"industry\"].values[0]\n",
        "    cluster_map.append((symbols[idx], f\"Cluster {winner_node[0]*som_y+winner_node[1]+1}\", industry))\n",
        "\n",
        "pd.DataFrame(cluster_map,columns=[\"Series\",\"Cluster\", \"Industry\"]).sort_values(by=\"Cluster\").set_index(\"Series\")"
      ],
      "metadata": {
        "id": "3611TKaGxVEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_map = []\n",
        "for idx in range(len(stock_prices_normal)):\n",
        "    winner_node = som.winner(stock_prices_normal[idx])\n",
        "    industry = companies_profiles[symbols[idx]][\"industry\"].values[0]\n",
        "    cluster_map.append((winner_node[0]*som_y+winner_node[1]+1, industry))\n",
        "\n",
        "pivot_df = pd.DataFrame(cluster_map,columns=[\"Cluster\", \"Industry\"]).sort_values(by=\"Cluster\")\n",
        "\n",
        "# Count unique industries per cluster\n",
        "unique_industries_per_cluster = pivot_df.groupby(\"Cluster\")[\"Industry\"].nunique().reset_index().sort_values(by=\"Cluster\")\n",
        "\n",
        "# Plotting the data using matplotlib.pyplot\n",
        "plt.figure(figsize=(25,5))\n",
        "plt.bar(unique_industries_per_cluster[\"Cluster\"], unique_industries_per_cluster[\"Industry\"],)\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.ylabel(\"Number of Unique Industries\")\n",
        "plt.title(\"Number of Unique Industries per Cluster\")\n",
        "plt.xticks(unique_industries_per_cluster[\"Cluster\"].tolist())\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E9htk_iijGXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### KMeans"
      ],
      "metadata": {
        "id": "UrvndUsrxKvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "winning_neurons = np.array([som.winner(x) for x in stock_prices_normal])\n",
        "\n",
        "kmeans_som = KMeans(n_clusters=23)\n",
        "clusters = kmeans_som.fit_predict(winning_neurons)\n"
      ],
      "metadata": {
        "id": "zH_EEp-YxONx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the performance of the model\n",
        "inertia = kmeans_som.inertia_\n",
        "print(\"Inertia:\", inertia)\n",
        "\n",
        "print(\"Silhouette Coefficient: %0.3f\"\n",
        "      % metrics.silhouette_score(stock_prices_normal, kmeans_som.labels_))\n",
        "\n",
        "print(\"Calinski-Harabasz Index: %0.3f\"\n",
        "      % metrics.calinski_harabasz_score(stock_prices_normal, kmeans_som.labels_))\n",
        "\n",
        "print(\"Davies-Bouldin Index: %0.3f\"\n",
        "      % metrics.davies_bouldin_score(stock_prices_normal, kmeans_som.labels_))"
      ],
      "metadata": {
        "id": "oQfQR7LnxcaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TimeSeriesKMeans Euclidian"
      ],
      "metadata": {
        "id": "bbBo4h__kyEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_kmeans_stats(km):\n",
        "  inertia = km.inertia_\n",
        "  print(\"Inertia:\", inertia)\n",
        "\n",
        "  print(\"Silhouette Coefficient: %0.3f\"\n",
        "        % metrics.silhouette_score(stock_prices_normal, km.labels_))\n",
        "\n",
        "  print(\"Calinski-Harabasz Index: %0.3f\"\n",
        "        % metrics.calinski_harabasz_score(stock_prices_normal, km.labels_))\n",
        "\n",
        "  print(\"Davies-Bouldin Index: %0.3f\"\n",
        "        % metrics.davies_bouldin_score(stock_prices_normal, km.labels_))\n",
        "\n",
        "  return [inertia,  metrics.silhouette_score(stock_prices_normal, km.labels_),\n",
        "          metrics.calinski_harabasz_score(stock_prices_normal, km.labels_),\n",
        "          metrics.davies_bouldin_score(stock_prices_normal, km.labels_)]"
      ],
      "metadata": {
        "id": "UgCZ5U-8O8rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run"
      ],
      "metadata": {
        "id": "PdZKU8wsk-oK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Apply TimeSeriesKMeans clustering with Euclidean metric\n",
        "kmeans_cluster_count = math.ceil(math.sqrt(len(stock_prices_normal)))\n",
        "km_euclidean = TimeSeriesKMeans(n_clusters=kmeans_cluster_count, metric=\"euclidean\", max_iter=1000, n_init=2, n_jobs=-1, random_state=1)\n",
        "clusters_euclidean = km_euclidean.fit_predict(stock_prices_normal)"
      ],
      "metadata": {
        "id": "TLQ2SCxakymQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore"
      ],
      "metadata": {
        "id": "yBjU2VGc4sJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_map = []\n",
        "for idx in range(len(clusters_euclidean)):\n",
        "    winner_node = clusters_euclidean[idx]\n",
        "    industry = companies_profiles[symbols[idx]][\"industry\"].values[0]\n",
        "    cluster_map.append((winner_node + 1, industry))\n",
        "\n",
        "\n",
        "pivot_df = pd.DataFrame(cluster_map,columns=[\"Cluster\", \"Industry\"]).sort_values(by=\"Cluster\")\n",
        "\n",
        "# Count unique industries per cluster\n",
        "unique_industries_per_cluster = pivot_df.groupby(\"Cluster\")[\"Industry\"].nunique().reset_index().sort_values(by=\"Cluster\")\n",
        "\n",
        "# Plotting the data using matplotlib.pyplot\n",
        "plt.figure(figsize=(25,5))\n",
        "plt.bar(unique_industries_per_cluster[\"Cluster\"], unique_industries_per_cluster[\"Industry\"],)\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.ylabel(\"Number of Unique Industries\")\n",
        "plt.title(\"Number of Unique Industries per Cluster\")\n",
        "plt.xticks(unique_industries_per_cluster[\"Cluster\"].tolist())\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2Rge_Wcd0oAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_count = math.ceil(math.sqrt(kmeans_cluster_count))\n",
        "\n",
        "fig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25))\n",
        "row_i=0\n",
        "column_j=0\n",
        "# For each label there is,\n",
        "# plots every series with that label\n",
        "for label in set(clusters_euclidean):\n",
        "    cluster = []\n",
        "    for i in range(len(clusters_euclidean)):\n",
        "            if(clusters_euclidean[i]==label):\n",
        "                axs[row_i, column_j].plot(stock_prices_normal[i],c=\"gray\",alpha=0.4)\n",
        "                cluster.append(stock_prices_normal[i])\n",
        "    if len(cluster) > 0:\n",
        "        axs[row_i, column_j].plot(np.average(np.vstack(cluster),axis=0),c=\"red\")\n",
        "    axs[row_i, column_j].set_title(\"Cluster \"+str(row_i*plot_count+column_j + 1))\n",
        "    column_j+=1\n",
        "    if column_j%plot_count == 0:\n",
        "        row_i+=1\n",
        "        column_j=0\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7Izliinh4tSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ],
      "metadata": {
        "id": "wSwAGMIxlAD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for cluster in [7]:\n",
        "  plt.figure(figsize=(25,5))\n",
        "  plt.title(f\"Cluster {cluster + 1}\")\n",
        "  for index, clusterNo in enumerate(clusters_euclidean):\n",
        "    if clusterNo == cluster:\n",
        "      plt.plot(stock_prices_normal[index])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "wRTH2JZy8rjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_kmeans_stats(km_euclidean)"
      ],
      "metadata": {
        "id": "K6wIr-mIljAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for kmeans_cluster_count in range(3, 30):\n",
        "  km_euclidean = TimeSeriesKMeans(n_clusters=kmeans_cluster_count, metric=\"euclidean\", max_iter=10, n_init=2, n_jobs=-1, random_state=1)\n",
        "  clusters_euclidean = km_euclidean.fit_predict(stock_prices_normal)\n",
        "  print(kmeans_cluster_count)\n",
        "  results.append(print_kmeans_stats(km_euclidean))"
      ],
      "metadata": {
        "id": "jutfdsQ6PAUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot([result[1] for result in results])\n",
        "plt.title('Silhouette by Coefficient')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot([result[2] for result in results])\n",
        "plt.title('Calinski-Harabasz Index')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot([result[3] for result in results])\n",
        "plt.title('Davies-Bouldin Index')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jnQ7ul3nP2ZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "df = MinMaxScaler().fit_transform(pd.DataFrame([result[1] for result in results]))\n",
        "df1 = df.reshape(len(df))\n",
        "\n",
        "df = MinMaxScaler().fit_transform(pd.DataFrame([result[2] for result in results]))\n",
        "df2 = df.reshape(len(df))\n",
        "\n",
        "df = MinMaxScaler().fit_transform(pd.DataFrame([result[3] for result in results]))\n",
        "df3 = df.reshape(len(df))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(3, 30), df1, label='Silhouette by Coefficient')\n",
        "plt.plot(range(3, 30), df2, label='Calinski-Harabasz Index')\n",
        "plt.plot(range(3, 30), df3, label='Davies-Bouldin Index')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yprdLf73RBKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_c = [len(km_euclidean.labels_[km_euclidean.labels_==i]) for i in range(23)]\n",
        "cluster_n = [f\"C{i + 1}\" for i in range(23)]\n",
        "plt.figure(figsize=(25,5))\n",
        "plt.title(\"Cluster Distribution of stocks for KMeans Euclidean\")\n",
        "plt.bar(cluster_n,cluster_c)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J-WCLkzoCW_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TimeSeriesKMeans DTW"
      ],
      "metadata": {
        "id": "s31FvLu2iZ4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run"
      ],
      "metadata": {
        "id": "BePflxe0clMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "kmeans_dtw_cluster_count = math.ceil(math.sqrt(len(stock_prices_normal)))\n",
        "# A good rule of thumb is choosing k as the square root of the number of points in the training data set in kNN\n",
        "\n",
        "print(f\"{kmeans_dtw_cluster_count} clusters\")\n",
        "\n",
        "km_dtw = TimeSeriesKMeans(n_clusters=kmeans_dtw_cluster_count, max_iter_barycenter=1000, metric=\"dtw\",n_init=10,  random_state=1, n_jobs=5)\n",
        "\n",
        "# elbow method - to check for sweet spot number of clusters: play with init https://tslearn.readthedocs.io/en/stable/gen_modules/clustering/tslearn.clustering.TimeSeriesKMeans.html\n",
        "km_dtw.fit(stock_prices_normal)\n",
        "\n",
        "clusters_kmeans_dtw = km_dtw.predict(stock_prices_normal)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lk7NjvLrEVpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ],
      "metadata": {
        "id": "Qo-EmfWlh7ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the performance of the model\n",
        "inertia = km_dtw.inertia_\n",
        "print(\"Inertia:\", inertia)\n",
        "\n",
        "print(\"Silhouette Coefficient: %0.3f\"\n",
        "      % metrics.silhouette_score(stock_prices_normal, km_dtw.labels_))\n",
        "\n",
        "print(\"Calinski-Harabasz Index: %0.3f\"\n",
        "      % metrics.calinski_harabasz_score(stock_prices_normal, km_dtw.labels_))\n",
        "\n",
        "print(\"Davies-Bouldin Index: %0.3f\"\n",
        "      % metrics.davies_bouldin_score(stock_prices_normal, km_dtw.labels_))\n",
        "\n"
      ],
      "metadata": {
        "id": "64nIlGFBwGWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore"
      ],
      "metadata": {
        "id": "agpq0C5Ycbur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clusters"
      ],
      "metadata": {
        "id": "mpGyP0hdiDMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Average"
      ],
      "metadata": {
        "id": "HDzhSFZXlSoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_count = math.ceil(math.sqrt(kmeans_dtw_cluster_count))\n",
        "\n",
        "fig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25))\n",
        "row_i=0\n",
        "column_j=0\n",
        "# For each label there is,\n",
        "# plots every series with that label\n",
        "for label in set(clusters_kmeans_dtw):\n",
        "    cluster = []\n",
        "    for i in range(len(clusters_kmeans_dtw)):\n",
        "            if(clusters_kmeans_dtw[i]==label):\n",
        "                axs[row_i, column_j].plot(stock_prices_normal[i],c=\"gray\",alpha=0.4)\n",
        "                cluster.append(stock_prices_normal[i])\n",
        "    if len(cluster) > 0:\n",
        "        axs[row_i, column_j].plot(np.average(np.vstack(cluster),axis=0),c=\"red\")\n",
        "    axs[row_i, column_j].set_title(\"Cluster \"+str(row_i*plot_count+column_j + 1))\n",
        "    column_j+=1\n",
        "    if column_j%plot_count == 0:\n",
        "        row_i+=1\n",
        "        column_j=0\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i01vpGfoEYc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### DBA"
      ],
      "metadata": {
        "id": "YCaYdzr3lVLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "draw_with_bary(clusters_kmeans_dtw, kmeans_dtw_cluster_count)"
      ],
      "metadata": {
        "id": "BvTJT_8jEbo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cluster in range(len(km_dtw.labels_)):\n",
        "  plt.figure(figsize=(25,5))\n",
        "  plt.title(f\"Cluster {cluster + 1}\")\n",
        "  for index, clusterNo in enumerate(km_dtw.labels_):\n",
        "    if clusterNo == cluster:\n",
        "      plt.plot(stock_prices_normal[index])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "97MXIz3Z4bOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Outliars"
      ],
      "metadata": {
        "id": "UnRcR6uTHIRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index in range(len(stock_prices_normal)):\n",
        "  if km_dtw.labels_[index] == 14:\n",
        "    plt.figure(figsize=(25,5))\n",
        "    plt.title(f\"Cluster 15: {symbols[index]}\")\n",
        "    plt.plot(stock_prices_normal[index])\n",
        "    plt.show()\n",
        "\n",
        "plt.figure(figsize=(25,5))\n",
        "names = []\n",
        "for index in range(len(stock_prices_normal)):\n",
        "  if km_dtw.labels_[index] == 19:\n",
        "    plt.plot(stock_prices_normal[index])\n",
        "    names.append(symbols[index])\n",
        "\n",
        "plt.title(f\"{', '.join(names)}\")\n",
        "plt.legend(names)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PAIzQe3XF09q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Distributions"
      ],
      "metadata": {
        "id": "_QIIveSviMA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_c = [len(km_dtw.labels_[km_dtw.labels_==i]) for i in range(kmeans_dtw_cluster_count)]\n",
        "cluster_n = [f\"C{i + 1}\" for i in range(kmeans_dtw_cluster_count)]\n",
        "plt.figure(figsize=(25,5))\n",
        "plt.title(\"Cluster Distribution of stocks for KMeans DTW\")\n",
        "plt.bar(cluster_n,cluster_c)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Um-Qcr6MEfYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_map = []\n",
        "for idx in range(len(km_dtw.labels_)):\n",
        "    winner_node = km_dtw.labels_[idx]\n",
        "    industry = companies_profiles[symbols[idx]][\"industry\"].values[0]\n",
        "    cluster_map.append((winner_node + 1, industry))\n",
        "\n",
        "\n",
        "pivot_df = pd.DataFrame(cluster_map,columns=[\"Cluster\", \"Industry\"]).sort_values(by=\"Cluster\")\n",
        "\n",
        "# Count unique industries per cluster\n",
        "unique_industries_per_cluster = pivot_df.groupby(\"Cluster\")[\"Industry\"].nunique().reset_index().sort_values(by=\"Cluster\")\n",
        "\n",
        "# Plotting the data using matplotlib.pyplot\n",
        "plt.figure(figsize=(25,5))\n",
        "plt.bar(unique_industries_per_cluster[\"Cluster\"], unique_industries_per_cluster[\"Industry\"],)\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.ylabel(\"Number of Unique Industries\")\n",
        "plt.title(\"Number of Unique Industries per Cluster\")\n",
        "plt.xticks(unique_industries_per_cluster[\"Cluster\"].tolist())\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3JzK2ulw1WBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(zip(km_dtw.labels_, [companies_profiles[d][\"industry\"].values[0] for d in symbols], symbols), columns=[\"Cluster\", \"Industry\", \"Symbol\"])\n",
        "grouped = data.groupby(\"Cluster\")[\"Industry\"]\n",
        "\n",
        "pd.DataFrame(grouped.apply(list))"
      ],
      "metadata": {
        "id": "LUDj0Bo43xuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principal component analysis (PCA)"
      ],
      "metadata": {
        "id": "m-IZlkdJiQRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run"
      ],
      "metadata": {
        "id": "UYbePXLzcujH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "pca = PCA()\n",
        "mySeries_transformed = pca.fit_transform(stock_prices_normal)\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.figure()\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Variance (%)')\n",
        "plt.title('Explained Variance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GklNBQkVEjVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "\n",
        "mySeries_transformed = pca.fit_transform(stock_prices_normal)\n",
        "\n",
        "# The explained variance ratio of each component\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
        "\n",
        "# The cumulative sum of the explained variance ratio\n",
        "print(\"Cumulative sum of explained variance ratio:\", np.cumsum(pca.explained_variance_ratio_))"
      ],
      "metadata": {
        "id": "44SGNFU2mDKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore"
      ],
      "metadata": {
        "id": "ylfq8YyPcvh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25,10))\n",
        "plt.scatter(mySeries_transformed[:,0], mySeries_transformed[:,1], s=100)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YebNcHUjQjdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25,10))\n",
        "\n",
        "plt.scatter(mySeries_transformed[:, 0], mySeries_transformed[:, 1], s=300)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "plt.title(\"TimeSeriesKMeans Clustering\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S2jlSp4Xi7d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KMeans"
      ],
      "metadata": {
        "id": "sS8is3vvidFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run"
      ],
      "metadata": {
        "id": "CzUaUNXfcoJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans_cluster_count = math.ceil(math.sqrt(len(stock_prices_normal)))\n",
        "kmeans = TimeSeriesKMeans(n_clusters=kmeans_cluster_count, metric=\"dtw\",random_state=1)\n",
        "labels = kmeans.fit_predict(mySeries_transformed)\n",
        "print(f\"{kmeans_cluster_count} clusters\")"
      ],
      "metadata": {
        "id": "lmIdtgQxi-oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_map = []\n",
        "for idx in range(len(labels)):\n",
        "    winner_node = labels[idx]\n",
        "    industry = companies_profiles[symbols[idx]][\"industry\"].values[0]\n",
        "    cluster_map.append((winner_node + 1, industry))\n",
        "\n",
        "\n",
        "pivot_df = pd.DataFrame(cluster_map,columns=[\"Cluster\", \"Industry\"]).sort_values(by=\"Cluster\")\n",
        "\n",
        "# Count unique industries per cluster\n",
        "unique_industries_per_cluster = pivot_df.groupby(\"Cluster\")[\"Industry\"].nunique().reset_index().sort_values(by=\"Cluster\")\n",
        "\n",
        "# Plotting the data using matplotlib.pyplot\n",
        "plt.figure(figsize=(25,5))\n",
        "plt.bar(unique_industries_per_cluster[\"Cluster\"], unique_industries_per_cluster[\"Industry\"],)\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.ylabel(\"Number of Unique Industries\")\n",
        "plt.title(\"Number of Unique Industries per Cluster\")\n",
        "plt.xticks(unique_industries_per_cluster[\"Cluster\"].tolist())\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Yb0x7RbR17NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate"
      ],
      "metadata": {
        "id": "R-E5sB_Ov8j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Silhouette Coefficient: %0.3f\"\n",
        "      % metrics.silhouette_score(stock_prices_normal, kmeans.labels_))\n",
        "\n",
        "print(\"Calinski-Harabasz Index: %0.3f\"\n",
        "      % metrics.calinski_harabasz_score(stock_prices_normal, kmeans.labels_))\n",
        "\n",
        "print(\"Davies-Bouldin Index: %0.3f\"\n",
        "      % metrics.davies_bouldin_score(stock_prices_normal, kmeans.labels_))"
      ],
      "metadata": {
        "id": "t1b-Nmu8v9ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explore"
      ],
      "metadata": {
        "id": "Ptfm-vg3cqVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Clusters"
      ],
      "metadata": {
        "id": "ATsGC29pming"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,15))\n",
        "plt.scatter(mySeries_transformed[:, 0], mySeries_transformed[:, 1], c=labels, s=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_nmCNP1NQ3CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cluster in range(0, kmeans_cluster_count):\n",
        "  plt.figure(figsize=(25,5))\n",
        "  plt.title(f\"Cluster {cluster + 1}\")\n",
        "  for index, clusterNo in enumerate(labels):\n",
        "    if clusterNo == cluster:\n",
        "      plt.plot(stock_prices_normal[index])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "DKlExW09mk3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Averaging Centre"
      ],
      "metadata": {
        "id": "_2dD7vNJhG0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_count = math.ceil(math.sqrt(kmeans_cluster_count))\n",
        "\n",
        "fig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25))\n",
        "row_i=0\n",
        "column_j=0\n",
        "for label in set(labels):\n",
        "    cluster = []\n",
        "    for i in range(len(labels)):\n",
        "            if(labels[i]==label):\n",
        "                axs[row_i, column_j].plot(stock_prices_normal[i],c=\"gray\",alpha=0.4)\n",
        "                cluster.append(stock_prices_normal[i])\n",
        "    if len(cluster) > 0:\n",
        "        axs[row_i, column_j].plot(np.average(np.vstack(cluster),axis=0),c=\"red\")\n",
        "    axs[row_i, column_j].set_title(\"Cluster \"+str(row_i*som_y+column_j))\n",
        "    column_j+=1\n",
        "    if column_j%plot_count == 0:\n",
        "        row_i+=1\n",
        "        column_j=0\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hSspShg3EskE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DBA"
      ],
      "metadata": {
        "id": "-5f_OujLTT6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_count = math.ceil(math.sqrt(kmeans_dtw_cluster_count))\n",
        "\n",
        "fig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25))\n",
        "row_i=0\n",
        "column_j=0\n",
        "for label in set(labels):\n",
        "    cluster = []\n",
        "    for i in range(len(labels)):\n",
        "            if(labels[i]==label):\n",
        "                axs[row_i, column_j].plot(stock_prices_normal[i],c=\"gray\",alpha=0.4)\n",
        "                cluster.append(stock_prices_normal[i])\n",
        "    if len(cluster) > 0:\n",
        "        axs[row_i, column_j].plot(dtw_barycenter_averaging(np.vstack(cluster)),c=\"red\")\n",
        "    axs[row_i, column_j].set_title(\"Cluster \"+str(row_i*plot_count+column_j + 1))\n",
        "    column_j+=1\n",
        "    if column_j%plot_count == 0:\n",
        "        row_i+=1\n",
        "        column_j=0\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yeqIFXrQTWmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Distributions"
      ],
      "metadata": {
        "id": "unVA_QKUhPDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_c = [len(labels[labels==i]) for i in range(kmeans_cluster_count)]\n",
        "cluster_n = [str(i + 1) for i in range(kmeans_cluster_count)]\n",
        "plt.figure(figsize=(25,5))\n",
        "plt.title(\"Cluster Distribution for PCA KMeans DTW\")\n",
        "plt.bar(cluster_n,cluster_c)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iIAojaFw7yz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fancy_names_for_labels = [f\"Cluster {label}\" for label in labels]\n",
        "pd.DataFrame(zip(symbols,fancy_names_for_labels, [companies_profiles[d][\"industry\"].values[0] for d in symbols]),\n",
        "             columns=[\"Series\",\"Cluster\", \"Industry\"]).sort_values(by=\"Cluster\").set_index(\"Series\")"
      ],
      "metadata": {
        "id": "TQxSem2KExaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison"
      ],
      "metadata": {
        "id": "mru4l3INiiq0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vh2VJsNIVYNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions"
      ],
      "metadata": {
        "id": "Ekmq26HJE9oA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HVdnXiqmE_1U"
      }
    }
  ]
}